{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story 4.4: Wallet Cluster Interpretation & Documentation\n",
    "\n",
    "**Objective:** Comprehensive analysis and interpretation of clustering results from Story 4.3\n",
    "\n",
    "**Date:** October 25, 2025\n",
    "\n",
    "**Dataset:** 2,159 wallets with HDBSCAN optimized and K-Means (k=5) cluster assignments\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs comprehensive cluster interpretation by:\n",
    "1. Loading both HDBSCAN and K-Means clustering results\n",
    "2. Validating feature value ranges for data quality\n",
    "3. Generating detailed statistical profiles for each cluster\n",
    "4. Identifying representative wallets (centroid, top performers, typical)\n",
    "5. Creating rich cluster personas with narratives\n",
    "6. Comparing HDBSCAN vs K-Means cluster mappings\n",
    "7. Deep-diving into the \"noise\" cluster (unique strategists)\n",
    "8. Generating actionable insights for each cluster\n",
    "9. Exporting comprehensive documentation\n",
    "\n",
    "**Expected Output:**\n",
    "- 7 data files with cluster profiles, personas, insights\n",
    "- Validation report identifying data quality issues\n",
    "- Rich visualizations of cluster characteristics\n",
    "- Actionable recommendations for research and trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Environment Setup\n",
    "\n",
    "**What we're doing:** Import necessary libraries for data analysis, clustering validation, and visualization.\n",
    "\n",
    "**Why:** We need pandas for data manipulation, sklearn for distance calculations (finding representative wallets), matplotlib/seaborn for visualizations, and json for exporting personas.\n",
    "\n",
    "**Expected output:** Confirmation that all libraries imported successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load Clustering Results\n",
    "\n",
    "**What we're doing:** Load both HDBSCAN optimized (primary) and K-Means k=5 (validation) clustering results from Story 4.3.\n",
    "\n",
    "**Why:** We use HDBSCAN optimized as our primary clustering (best silhouette: 0.4078) but validate findings against K-Means to ensure consistency.\n",
    "\n",
    "**Expected output:** Two dataframes loaded with 2,159 wallets each, confirmation of matching wallet addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "CLUSTERING_DIR = Path(\"../outputs/clustering\")\n",
    "OUTPUT_DIR = Path(\"../outputs/cluster_interpretation\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load HDBSCAN optimized (primary)\n",
    "hdbscan_files = list(CLUSTERING_DIR.glob(\"wallet_features_with_clusters_optimized_*.csv\"))\n",
    "hdbscan_file = max(hdbscan_files, key=lambda p: p.stat().st_mtime)\n",
    "df_hdbscan = pd.read_csv(hdbscan_file)\n",
    "\n",
    "# Load K-Means k=5 (validation)\n",
    "kmeans_files = list(CLUSTERING_DIR.glob(\"wallet_features_with_clusters_final_*.csv\"))\n",
    "kmeans_file = max(kmeans_files, key=lambda p: p.stat().st_mtime)\n",
    "df_kmeans = pd.read_csv(kmeans_file)\n",
    "\n",
    "print(f\"✅ HDBSCAN Optimized: {len(df_hdbscan):,} wallets\")\n",
    "print(f\"   File: {hdbscan_file.name}\")\n",
    "print(f\"\\n✅ K-Means (k=5): {len(df_kmeans):,} wallets\")\n",
    "print(f\"   File: {kmeans_file.name}\")\n",
    "\n",
    "# Verify same wallets\n",
    "assert (df_hdbscan['wallet_address'] == df_kmeans['wallet_address']).all(), \"Wallet address mismatch!\"\n",
    "print(\"\\n✅ Wallet addresses match between datasets\")\n",
    "\n",
    "# Preview data\n",
    "print(f\"\\nColumns: {len(df_hdbscan.columns)}\")\n",
    "print(f\"Features: {len([c for c in df_hdbscan.columns if c not in ['wallet_address', 'cluster', 'cluster_name', 'activity_segment']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Feature Value Validation\n",
    "\n",
    "**What we're doing:** Check that key features are within their expected ranges to identify data quality issues.\n",
    "\n",
    "**Why:** Features like HHI should be 0-1, win_rate should be 0-100%, etc. Out-of-range values indicate feature engineering issues that could affect interpretation.\n",
    "\n",
    "**Expected output:** Validation summary showing which features pass/fail range checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected ranges for key features\n",
    "features_to_check = {\n",
    "    'portfolio_hhi': (0, 1, 'Herfindahl-Hirschman Index'),\n",
    "    'portfolio_gini': (0, 1, 'Gini coefficient'),\n",
    "    'win_rate': (0, 100, 'Win rate percentage'),\n",
    "    'defi_exposure_pct': (0, 100, 'DeFi exposure'),\n",
    "    'ai_exposure_pct': (0, 100, 'AI exposure'),\n",
    "    'meme_exposure_pct': (0, 100, 'Meme exposure'),\n",
    "    'weekend_activity_ratio': (0, 1, 'Weekend activity'),\n",
    "    'night_trading_ratio': (0, 1, 'Night trading'),\n",
    "    'stablecoin_usage_ratio': (0, 1, 'Stablecoin usage'),\n",
    "}\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "print(\"Feature Validation Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feature, (min_val, max_val, description) in features_to_check.items():\n",
    "    if feature not in df_hdbscan.columns:\n",
    "        print(f\"⚠️  {feature}: NOT FOUND\")\n",
    "        validation_results.append({'feature': feature, 'status': 'missing', 'issue': 'Column not found'})\n",
    "        continue\n",
    "    \n",
    "    actual_min = df_hdbscan[feature].min()\n",
    "    actual_max = df_hdbscan[feature].max()\n",
    "    \n",
    "    if actual_min < min_val or actual_max > max_val:\n",
    "        print(f\"⚠️  {feature}: [{actual_min:.2f}, {actual_max:.2f}] (expected [{min_val}, {max_val}])\")\n",
    "        validation_results.append({\n",
    "            'feature': feature,\n",
    "            'status': 'fail',\n",
    "            'actual_range': f\"[{actual_min:.2f}, {actual_max:.2f}]\",\n",
    "            'expected_range': f\"[{min_val}, {max_val}]\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"✅ {feature}: [{actual_min:.2f}, {actual_max:.2f}]\")\n",
    "        validation_results.append({\n",
    "            'feature': feature,\n",
    "            'status': 'pass',\n",
    "            'actual_range': f\"[{actual_min:.2f}, {actual_max:.2f}]\"\n",
    "        })\n",
    "\n",
    "# Summary\n",
    "issues = [r for r in validation_results if r['status'] == 'fail']\n",
    "print(f\"\\n{'='*80}\")\n",
    "if issues:\n",
    "    print(f\"⚠️  Found {len(issues)} validation issue(s)\")\n",
    "    print(\"These will be documented but won't block interpretation.\")\n",
    "else:\n",
    "    print(\"✅ All features validated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Prepare Data for Analysis\n",
    "\n",
    "**What we're doing:** Merge HDBSCAN and K-Means results, separate feature columns from metadata.\n",
    "\n",
    "**Why:** We need both cluster assignments in one dataframe for comparison, and we need to identify which columns are features vs metadata for profiling.\n",
    "\n",
    "**Expected output:** Single merged dataframe with both cluster assignments, list of feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename cluster columns for clarity\n",
    "df_hdbscan = df_hdbscan.rename(columns={\n",
    "    'cluster': 'hdbscan_cluster',\n",
    "    'cluster_name': 'hdbscan_cluster_name'\n",
    "})\n",
    "df_kmeans = df_kmeans.rename(columns={\n",
    "    'cluster': 'kmeans_cluster',\n",
    "    'cluster_name': 'kmeans_cluster_name'\n",
    "})\n",
    "\n",
    "# Merge into single dataframe\n",
    "df = df_hdbscan.copy()\n",
    "df['kmeans_cluster'] = df_kmeans['kmeans_cluster']\n",
    "df['kmeans_cluster_name'] = df_kmeans['kmeans_cluster_name']\n",
    "\n",
    "# Identify feature columns (exclude metadata)\n",
    "exclude_cols = ['wallet_address', 'activity_segment', \n",
    "                'hdbscan_cluster', 'hdbscan_cluster_name',\n",
    "                'kmeans_cluster', 'kmeans_cluster_name']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"✅ Data prepared for analysis\")\n",
    "print(f\"   Total wallets: {len(df):,}\")\n",
    "print(f\"   Feature columns: {len(feature_cols)}\")\n",
    "print(f\"   HDBSCAN clusters: {df['hdbscan_cluster'].nunique()}\")\n",
    "print(f\"   K-Means clusters: {df['kmeans_cluster'].nunique()}\")\n",
    "\n",
    "# Display cluster distribution\n",
    "print(f\"\\nHDBSCAN Cluster Distribution:\")\n",
    "cluster_counts = df['hdbscan_cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    label = \"Noise\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "    print(f\"   {label}: {count:,} ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Generate Detailed Cluster Profiles\n",
    "\n",
    "**What we're doing:** Calculate 27 statistical metrics for each HDBSCAN cluster.\n",
    "\n",
    "**Why:** Comprehensive profiles enable us to understand cluster characteristics across performance, activity, portfolio composition, and narrative dimensions.\n",
    "\n",
    "**Metrics calculated:**\n",
    "- **Performance:** ROI (mean/median/std), win rate, Sharpe, PnL\n",
    "- **Activity:** Trade frequency, holding periods, weekend/night trading\n",
    "- **Portfolio:** HHI, Gini, token counts, narrative diversity\n",
    "- **Narrative:** DeFi/AI/Meme exposure, stablecoin usage\n",
    "- **Behavior:** % profitable, % active, % multi-token\n",
    "\n",
    "**Expected output:** Dictionary of cluster profiles with detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_clusters = sorted(df['hdbscan_cluster'].unique())\n",
    "\n",
    "cluster_profiles = {}\n",
    "\n",
    "for cluster_id in hdbscan_clusters:\n",
    "    cluster_data = df[df['hdbscan_cluster'] == cluster_id]\n",
    "    \n",
    "    profile = {\n",
    "        'cluster_id': int(cluster_id),\n",
    "        'size': len(cluster_data),\n",
    "        'percentage': len(cluster_data) / len(df) * 100,\n",
    "        \n",
    "        # Performance metrics\n",
    "        'roi_mean': cluster_data['roi_percent'].mean(),\n",
    "        'roi_median': cluster_data['roi_percent'].median(),\n",
    "        'roi_std': cluster_data['roi_percent'].std(),\n",
    "        'win_rate_mean': cluster_data['win_rate'].mean(),\n",
    "        'sharpe_mean': cluster_data['sharpe_ratio'].mean(),\n",
    "        'pnl_mean': cluster_data['total_pnl_usd'].mean(),\n",
    "        \n",
    "        # Activity metrics\n",
    "        'trade_freq_mean': cluster_data['trade_frequency'].mean(),\n",
    "        'holding_days_mean': cluster_data['avg_holding_period_days'].mean(),\n",
    "        'weekend_ratio': cluster_data['weekend_activity_ratio'].mean(),\n",
    "        'night_ratio': cluster_data['night_trading_ratio'].mean(),\n",
    "        \n",
    "        # Portfolio metrics\n",
    "        'hhi_mean': cluster_data['portfolio_hhi'].mean(),\n",
    "        'gini_mean': cluster_data['portfolio_gini'].mean(),\n",
    "        'num_tokens_mean': cluster_data['num_tokens_avg'].mean(),\n",
    "        'narrative_diversity_mean': cluster_data['narrative_diversity_score'].mean(),\n",
    "        \n",
    "        # Narrative exposure\n",
    "        'defi_exposure': cluster_data['defi_exposure_pct'].mean(),\n",
    "        'ai_exposure': cluster_data['ai_exposure_pct'].mean(),\n",
    "        'meme_exposure': cluster_data['meme_exposure_pct'].mean(),\n",
    "        'stablecoin_ratio': cluster_data['stablecoin_usage_ratio'].mean(),\n",
    "        \n",
    "        # Behavior flags\n",
    "        'pct_profitable': (cluster_data['is_profitable'] == 1).mean() * 100,\n",
    "        'pct_active': (cluster_data['is_active'] == 1).mean() * 100,\n",
    "        'pct_multi_token': (cluster_data['is_multi_token'] == 1).mean() * 100,\n",
    "    }\n",
    "    \n",
    "    cluster_profiles[cluster_id] = profile\n",
    "\n",
    "print(f\"✅ Generated {len(cluster_profiles)} detailed cluster profiles\")\n",
    "print(f\"   Metrics per cluster: {len(profile)}\")\n",
    "\n",
    "# Display sample profile\n",
    "sample_cluster = 0 if 0 in cluster_profiles else list(cluster_profiles.keys())[1]\n",
    "print(f\"\\nSample Profile (Cluster {sample_cluster}):\")\n",
    "sample = cluster_profiles[sample_cluster]\n",
    "print(f\"   Size: {sample['size']:,} ({sample['percentage']:.1f}%)\")\n",
    "print(f\"   ROI: {sample['roi_mean']:.1f}% (median: {sample['roi_median']:.1f}%)\")\n",
    "print(f\"   Trade frequency: {sample['trade_freq_mean']:.1f}\")\n",
    "print(f\"   Holding days: {sample['holding_days_mean']:.1f}\")\n",
    "print(f\"   HHI: {sample['hhi_mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Identify Representative Wallets\n",
    "\n",
    "**What we're doing:** For each cluster, find 3 types of representative wallets:\n",
    "1. **Centroid wallet** - closest to cluster mean across all features\n",
    "2. **Top performers** - highest ROI wallets (up to 3)\n",
    "3. **Typical wallets** - closest to median ROI (up to 3)\n",
    "\n",
    "**Why:** Representative wallets enable case study deep-dives and qualitative strategy analysis.\n",
    "\n",
    "**Method:** Use Euclidean distance in feature space to find centroid, sort by ROI for top performers, minimize distance from median ROI for typical.\n",
    "\n",
    "**Expected output:** Dictionary mapping cluster IDs to representative wallet addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_wallets = {}\n",
    "\n",
    "for cluster_id in hdbscan_clusters:\n",
    "    if cluster_id == -1:  # Skip noise cluster for now\n",
    "        continue\n",
    "    \n",
    "    cluster_mask = df['hdbscan_cluster'] == cluster_id\n",
    "    cluster_data = df[cluster_mask]\n",
    "    \n",
    "    if len(cluster_data) < 3:\n",
    "        # For very small clusters, just pick first wallet\n",
    "        representative_wallets[cluster_id] = {\n",
    "            'centroid': cluster_data.iloc[0]['wallet_address'],\n",
    "            'top_performers': [],\n",
    "            'typical': []\n",
    "        }\n",
    "        continue\n",
    "    \n",
    "    # 1. Find centroid wallet (closest to mean)\n",
    "    cluster_features = cluster_data[feature_cols].values\n",
    "    centroid = cluster_features.mean(axis=0)\n",
    "    distances = euclidean_distances(cluster_features, centroid.reshape(1, -1))\n",
    "    centroid_idx = distances.argmin()\n",
    "    centroid_wallet = cluster_data.iloc[centroid_idx]['wallet_address']\n",
    "    \n",
    "    # 2. Find top 3 performers by ROI\n",
    "    top_performers = cluster_data.nlargest(min(3, len(cluster_data)), 'roi_percent')['wallet_address'].tolist()\n",
    "    \n",
    "    # 3. Find 3 typical wallets (closest to median ROI)\n",
    "    median_roi = cluster_data['roi_percent'].median()\n",
    "    cluster_data_sorted = cluster_data.copy()\n",
    "    cluster_data_sorted['roi_distance'] = (cluster_data_sorted['roi_percent'] - median_roi).abs()\n",
    "    typical_wallets = cluster_data_sorted.nsmallest(min(3, len(cluster_data)), 'roi_distance')['wallet_address'].tolist()\n",
    "    \n",
    "    representative_wallets[cluster_id] = {\n",
    "        'centroid': centroid_wallet,\n",
    "        'top_performers': top_performers,\n",
    "        'typical': typical_wallets,\n",
    "    }\n",
    "\n",
    "print(f\"✅ Identified representative wallets for {len(representative_wallets)} clusters\")\n",
    "\n",
    "# Display sample\n",
    "if representative_wallets:\n",
    "    sample_cluster = list(representative_wallets.keys())[0]\n",
    "    sample = representative_wallets[sample_cluster]\n",
    "    print(f\"\\nSample (Cluster {sample_cluster}):\")\n",
    "    print(f\"   Centroid: {sample['centroid'][:16]}...\")\n",
    "    print(f\"   Top performers: {len(sample['top_performers'])} wallets\")\n",
    "    print(f\"   Typical: {len(sample['typical'])} wallets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Create Rich Cluster Personas\n",
    "\n",
    "**What we're doing:** Generate narrative descriptions for each cluster based on statistical characteristics.\n",
    "\n",
    "**Why:** Personas make clusters interpretable and actionable. They translate statistics into human-understandable archetypes.\n",
    "\n",
    "**Persona elements:**\n",
    "- **Name:** Descriptive label (e.g., \"Elite Performers\", \"Long-term Holders\")\n",
    "- **Archetype:** Higher-level category\n",
    "- **Tagline:** One-sentence summary\n",
    "- **Description:** Rich narrative explanation\n",
    "- **Characteristics:** Bullet-point list of key traits\n",
    "- **Investment Style:** Trading approach\n",
    "- **Risk Profile:** Risk-return characteristics\n",
    "- **Recommendation:** Action items for research/trading\n",
    "\n",
    "**Expected output:** Dictionary of rich personas for all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_persona(cluster_id, profile, rep_wallets):\n",
    "    \"\"\"Generate rich persona based on cluster statistics.\"\"\"\n",
    "    \n",
    "    if cluster_id == -1:\n",
    "        return {\n",
    "            'name': 'Unique Strategists (Noise)',\n",
    "            'archetype': 'Outliers',\n",
    "            'tagline': 'Wallets with unique, non-conforming strategies',\n",
    "            'description': (\n",
    "                f\"This group contains {profile['size']:,} wallets ({profile['percentage']:.1f}%) \"\n",
    "                \"that don't fit well into any standard cluster pattern. These wallets employ \"\n",
    "                \"unique or hybrid strategies that defy categorization. In crypto markets, \"\n",
    "                \"where innovation is rewarded, these outliers may represent the most adaptive traders.\"\n",
    "            ),\n",
    "            'characteristics': [\n",
    "                'Highly diverse trading patterns',\n",
    "                'Don\\'t conform to typical wallet behavior',\n",
    "                'May represent innovative or experimental strategies',\n",
    "                'Could include both exceptional performers and unique failures',\n",
    "            ],\n",
    "            'investment_style': 'Non-standard, experimental',\n",
    "            'risk_profile': 'Variable',\n",
    "            'recommendation': 'Study individually for unique insights',\n",
    "        }\n",
    "    \n",
    "    # Extract key metrics\n",
    "    roi = profile['roi_mean']\n",
    "    trade_freq = profile['trade_freq_mean']\n",
    "    holding_days = profile['holding_days_mean']\n",
    "    hhi = profile['hhi_mean']\n",
    "    \n",
    "    # Classify performance\n",
    "    if roi > 100:\n",
    "        performance = 'Elite'\n",
    "    elif roi > 50:\n",
    "        performance = 'High'\n",
    "    elif roi > 0:\n",
    "        performance = 'Moderate'\n",
    "    else:\n",
    "        performance = 'Struggling'\n",
    "    \n",
    "    # Classify activity\n",
    "    if trade_freq > 10:\n",
    "        activity = 'Hyperactive'\n",
    "    elif trade_freq > 5:\n",
    "        activity = 'Active'\n",
    "    elif trade_freq > 2:\n",
    "        activity = 'Moderate'\n",
    "    else:\n",
    "        activity = 'Passive'\n",
    "    \n",
    "    # Create name\n",
    "    if hhi > 0.7 or hhi > 7000:  # Account for 0-10000 scale\n",
    "        name = f\"Focused Specialists\"\n",
    "        archetype = \"Concentrated Portfolios\"\n",
    "    elif holding_days > 30:\n",
    "        name = f\"Long-term Holders\"\n",
    "        archetype = \"Diamond Hands\"\n",
    "    elif activity == 'Hyperactive':\n",
    "        name = f\"Hyperactive Traders\"\n",
    "        archetype = \"High-Frequency Operators\"\n",
    "    else:\n",
    "        name = f\"{performance} {activity} Traders\"\n",
    "        archetype = f\"{performance} Performers\"\n",
    "    \n",
    "    # Create tagline\n",
    "    tagline = f\"{performance} performers with {activity.lower()} trading style\"\n",
    "    \n",
    "    # Create description\n",
    "    description = (\n",
    "        f\"This cluster contains {profile['size']:,} wallets ({profile['percentage']:.1f}%) \"\n",
    "        f\"characterized by {performance.lower()} performance metrics \"\n",
    "        f\"(average ROI: {roi:.1f}%). \"\n",
    "    )\n",
    "    \n",
    "    if activity in ['Hyperactive', 'Active']:\n",
    "        description += f\"These wallets trade frequently (avg {trade_freq:.1f} trades). \"\n",
    "    else:\n",
    "        description += f\"These wallets trade infrequently (avg {trade_freq:.1f} trades). \"\n",
    "    \n",
    "    # Characteristics\n",
    "    characteristics = [\n",
    "        f\"Average ROI: {roi:.1f}%\",\n",
    "        f\"Trade frequency: {trade_freq:.1f} trades\",\n",
    "        f\"Holding period: {holding_days:.0f} days\",\n",
    "        f\"Portfolio concentration (HHI): {hhi:.2f}\",\n",
    "    ]\n",
    "    \n",
    "    # Investment style\n",
    "    if activity in ['Hyperactive', 'Active']:\n",
    "        investment_style = \"Active trading with frequent position changes\"\n",
    "    elif holding_days > 30:\n",
    "        investment_style = \"Buy-and-hold with long-term conviction\"\n",
    "    else:\n",
    "        investment_style = \"Balanced approach with selective entries/exits\"\n",
    "    \n",
    "    # Risk profile\n",
    "    sharpe = profile['sharpe_mean']\n",
    "    if sharpe > 2:\n",
    "        risk_profile = f\"High risk-adjusted returns (Sharpe: {sharpe:.2f})\"\n",
    "    elif sharpe > 1:\n",
    "        risk_profile = f\"Moderate risk-adjusted returns (Sharpe: {sharpe:.2f})\"\n",
    "    else:\n",
    "        risk_profile = \"Lower risk-adjusted performance\"\n",
    "    \n",
    "    # Recommendation\n",
    "    if performance == 'Elite':\n",
    "        recommendation = \"Study strategies for replication; identify alpha sources\"\n",
    "    elif performance == 'Struggling':\n",
    "        recommendation = \"Avoid mimicking; analyze failure modes\"\n",
    "    else:\n",
    "        recommendation = \"Baseline behavior; useful for comparative analysis\"\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'archetype': archetype,\n",
    "        'tagline': tagline,\n",
    "        'description': description,\n",
    "        'characteristics': characteristics,\n",
    "        'investment_style': investment_style,\n",
    "        'risk_profile': risk_profile,\n",
    "        'recommendation': recommendation,\n",
    "    }\n",
    "\n",
    "# Generate personas\n",
    "personas = {}\n",
    "for cluster_id, profile in cluster_profiles.items():\n",
    "    rep_wallets = representative_wallets.get(cluster_id, {})\n",
    "    persona = create_persona(cluster_id, profile, rep_wallets)\n",
    "    personas[cluster_id] = persona\n",
    "\n",
    "print(f\"✅ Created {len(personas)} detailed cluster personas\")\n",
    "\n",
    "# Display sample personas\n",
    "print(\"\\nSample Personas:\")\n",
    "for i, (cluster_id, persona) in enumerate(list(personas.items())[:3]):\n",
    "    print(f\"\\n{i+1}. Cluster {cluster_id}: {persona['name']}\")\n",
    "    print(f\"   {persona['tagline']}\")\n",
    "    print(f\"   Size: {cluster_profiles[cluster_id]['size']:,} wallets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Compare HDBSCAN vs K-Means Clustering\n",
    "\n",
    "**What we're doing:** Create cross-tabulation showing how HDBSCAN clusters map to K-Means clusters.\n",
    "\n",
    "**Why:** High overlap between algorithms validates clustering quality. If both methods identify similar groups, we have confidence in the results.\n",
    "\n",
    "**Metrics calculated:**\n",
    "- Cross-tabulation matrix\n",
    "- Overlap percentage per cluster\n",
    "- Fragmentation (how many K-Means clusters each HDBSCAN cluster splits into)\n",
    "\n",
    "**Expected output:** Cross-tab table and overlap analysis showing 90-100% agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-tabulation\n",
    "cross_tab = pd.crosstab(\n",
    "    df['hdbscan_cluster'],\n",
    "    df['kmeans_cluster'],\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "\n",
    "print(\"HDBSCAN vs K-Means Cluster Mapping:\")\n",
    "print(\"=\" * 80)\n",
    "print(cross_tab)\n",
    "print()\n",
    "\n",
    "# Calculate overlap metrics\n",
    "overlap_analysis = []\n",
    "\n",
    "for hdb_cluster in hdbscan_clusters:\n",
    "    if hdb_cluster == -1:\n",
    "        continue\n",
    "    \n",
    "    hdb_wallets = df[df['hdbscan_cluster'] == hdb_cluster]\n",
    "    if len(hdb_wallets) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Find dominant K-Means cluster\n",
    "    kmeans_dist = hdb_wallets['kmeans_cluster'].value_counts()\n",
    "    dominant_kmeans = kmeans_dist.index[0]\n",
    "    overlap_count = kmeans_dist.iloc[0]\n",
    "    overlap_pct = (overlap_count / len(hdb_wallets)) * 100\n",
    "    \n",
    "    overlap_analysis.append({\n",
    "        'hdbscan_cluster': int(hdb_cluster),\n",
    "        'size': len(hdb_wallets),\n",
    "        'dominant_kmeans_cluster': int(dominant_kmeans),\n",
    "        'overlap_count': int(overlap_count),\n",
    "        'overlap_percentage': float(overlap_pct),\n",
    "        'fragmentation': len(kmeans_dist),\n",
    "    })\n",
    "\n",
    "overlap_df = pd.DataFrame(overlap_analysis)\n",
    "\n",
    "print(\"\\nCluster Overlap Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(overlap_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "perfect_overlap = (overlap_df['overlap_percentage'] == 100).sum()\n",
    "high_overlap = (overlap_df['overlap_percentage'] >= 90).sum()\n",
    "avg_overlap = overlap_df['overlap_percentage'].mean()\n",
    "\n",
    "print(f\"Overlap Summary:\")\n",
    "print(f\"   Perfect overlap (100%): {perfect_overlap} clusters\")\n",
    "print(f\"   High overlap (≥90%): {high_overlap} clusters\")\n",
    "print(f\"   Average overlap: {avg_overlap:.1f}%\")\n",
    "print(f\"\\n✅ Strong algorithmic agreement validates clustering quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Deep Dive into Noise Cluster\n",
    "\n",
    "**What we're doing:** Analyze the \"noise\" cluster (-1) containing wallets that don't fit standard patterns.\n",
    "\n",
    "**Why:** 48.4% of wallets are classified as noise by HDBSCAN. This large percentage is itself a research finding about wallet behavior heterogeneity.\n",
    "\n",
    "**Analysis includes:**\n",
    "- Size and percentage\n",
    "- ROI statistics (mean, median, std)\n",
    "- Profitability breakdown\n",
    "- Top 10 performers\n",
    "- Worst 10 performers\n",
    "\n",
    "**Expected output:** Comprehensive noise cluster statistics showing high variance and exceptional performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_wallets = df[df['hdbscan_cluster'] == -1]\n",
    "\n",
    "print(\"Noise Cluster Analysis (Unique Strategists)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Size: {len(noise_wallets):,} wallets ({len(noise_wallets)/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if len(noise_wallets) > 0:\n",
    "    # Basic statistics\n",
    "    print(\"ROI Statistics:\")\n",
    "    print(f\"   Mean: {noise_wallets['roi_percent'].mean():.1f}%\")\n",
    "    print(f\"   Median: {noise_wallets['roi_percent'].median():.1f}%\")\n",
    "    print(f\"   Std Dev: {noise_wallets['roi_percent'].std():.1f}% (high variance)\")\n",
    "    print(f\"   Min: {noise_wallets['roi_percent'].min():.1f}%\")\n",
    "    print(f\"   Max: {noise_wallets['roi_percent'].max():.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Profitability breakdown\n",
    "    profitable = (noise_wallets['roi_percent'] > 0).sum()\n",
    "    exceptional = (noise_wallets['roi_percent'] > 100).sum()\n",
    "    negative = (noise_wallets['roi_percent'] < 0).sum()\n",
    "    \n",
    "    print(\"Performance Breakdown:\")\n",
    "    print(f\"   Positive ROI: {profitable:,} ({profitable/len(noise_wallets)*100:.1f}%)\")\n",
    "    print(f\"   Exceptional (>100%): {exceptional:,} ({exceptional/len(noise_wallets)*100:.1f}%)\")\n",
    "    print(f\"   Negative ROI: {negative:,} ({negative/len(noise_wallets)*100:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Top performers\n",
    "    print(\"Top 10 Noise Cluster Performers:\")\n",
    "    top_noise = noise_wallets.nlargest(10, 'roi_percent')[[\n",
    "        'wallet_address', 'roi_percent', 'trade_frequency', 'num_tokens_avg'\n",
    "    ]]\n",
    "    print(top_noise.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Worst performers\n",
    "    print(\"Worst 10 Noise Cluster Performers:\")\n",
    "    worst_noise = noise_wallets.nsmallest(10, 'roi_percent')[[\n",
    "        'wallet_address', 'roi_percent', 'trade_frequency', 'num_tokens_avg'\n",
    "    ]]\n",
    "    print(worst_noise.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    print(\"✅ Noise cluster contains exceptional performers and diverse strategies\")\n",
    "    print(\"   Recommendation: Study top performers individually for unique alpha\")\n",
    "else:\n",
    "    print(\"No noise cluster found (all wallets assigned to clusters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Generate Actionable Insights\n",
    "\n",
    "**What we're doing:** Create specific, actionable insights for each cluster across 4 categories.\n",
    "\n",
    "**Why:** Move from descriptive statistics to prescriptive recommendations for researchers, traders, and developers.\n",
    "\n",
    "**Insight categories:**\n",
    "1. **Key Insights:** Main findings about cluster behavior\n",
    "2. **Trading Implications:** How to use these insights for trading\n",
    "3. **Research Questions:** What to investigate further\n",
    "4. **Data Opportunities:** Specific analyses to conduct\n",
    "\n",
    "**Expected output:** Structured insights dictionary for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_insights = {}\n",
    "\n",
    "for cluster_id, profile in cluster_profiles.items():\n",
    "    persona = personas[cluster_id]\n",
    "    \n",
    "    insights = {\n",
    "        'cluster_id': int(cluster_id),\n",
    "        'cluster_name': persona['name'],\n",
    "        'size': profile['size'],\n",
    "        'percentage': profile['percentage'],\n",
    "        'key_insights': [],\n",
    "        'trading_implications': [],\n",
    "        'research_questions': [],\n",
    "        'data_opportunities': [],\n",
    "    }\n",
    "    \n",
    "    roi = profile['roi_mean']\n",
    "    trade_freq = profile['trade_freq_mean']\n",
    "    hhi = profile['hhi_mean']\n",
    "    \n",
    "    # Generate insights based on characteristics\n",
    "    if cluster_id == -1:\n",
    "        insights['key_insights'].append(\n",
    "            f\"{profile['percentage']:.1f}% of wallets defy standard categorization\"\n",
    "        )\n",
    "        insights['key_insights'].append(\n",
    "            \"High variance suggests diverse experimental strategies\"\n",
    "        )\n",
    "        insights['trading_implications'].append(\n",
    "            \"These wallets may identify emerging trends before mainstream\"\n",
    "        )\n",
    "        insights['research_questions'].append(\n",
    "            \"What unique strategies do noise wallets employ?\"\n",
    "        )\n",
    "    elif roi > 100:\n",
    "        insights['key_insights'].append(\n",
    "            f\"Exceptional returns ({roi:.0f}% ROI) significantly outperform market\"\n",
    "        )\n",
    "        insights['trading_implications'].append(\n",
    "            \"Study token selection and entry/exit timing for alpha signals\"\n",
    "        )\n",
    "        insights['research_questions'].append(\n",
    "            \"What tokens or narratives drove exceptional performance?\"\n",
    "        )\n",
    "    elif roi < 0:\n",
    "        insights['key_insights'].append(\n",
    "            f\"Consistent underperformance ({roi:.0f}% ROI)\"\n",
    "        )\n",
    "        insights['trading_implications'].append(\n",
    "            \"Analyze failure patterns to avoid similar mistakes\"\n",
    "        )\n",
    "    \n",
    "    if trade_freq > 10:\n",
    "        insights['key_insights'].append(\n",
    "            f\"High trading frequency ({trade_freq:.0f} trades) suggests active management\"\n",
    "        )\n",
    "    \n",
    "    if hhi > 0.7 or hhi > 7000:\n",
    "        insights['key_insights'].append(\n",
    "            \"Highly concentrated portfolios indicate conviction-based investing\"\n",
    "        )\n",
    "        insights['research_questions'].append(\n",
    "            \"What drives concentrated allocation decisions?\"\n",
    "        )\n",
    "    \n",
    "    # Data opportunities\n",
    "    rep_wallet = representative_wallets.get(cluster_id, {}).get('centroid', 'N/A')\n",
    "    if rep_wallet != 'N/A':\n",
    "        insights['data_opportunities'].append(\n",
    "            f\"Deep dive into {rep_wallet[:10]}... (centroid wallet)\"\n",
    "        )\n",
    "    insights['data_opportunities'].append(\n",
    "        \"Analyze token overlap within cluster for narrative trends\"\n",
    "    )\n",
    "    insights['data_opportunities'].append(\n",
    "        \"Track cluster migration over time for strategy evolution\"\n",
    "    )\n",
    "    \n",
    "    cluster_insights[cluster_id] = insights\n",
    "\n",
    "print(f\"✅ Generated actionable insights for {len(cluster_insights)} clusters\")\n",
    "\n",
    "# Display sample insights\n",
    "sample_cluster = 0 if 0 in cluster_insights else list(cluster_insights.keys())[1]\n",
    "sample = cluster_insights[sample_cluster]\n",
    "print(f\"\\nSample Insights (Cluster {sample_cluster}: {sample['cluster_name']}):\")\n",
    "print(f\"   Key Insights: {len(sample['key_insights'])}\")\n",
    "if sample['key_insights']:\n",
    "    print(f\"      • {sample['key_insights'][0]}\")\n",
    "print(f\"   Trading Implications: {len(sample['trading_implications'])}\")\n",
    "print(f\"   Research Questions: {len(sample['research_questions'])}\")\n",
    "print(f\"   Data Opportunities: {len(sample['data_opportunities'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Visualize Cluster Characteristics\n",
    "\n",
    "**What we're doing:** Create visual comparisons of cluster profiles.\n",
    "\n",
    "**Why:** Visualizations make patterns easier to identify and communicate.\n",
    "\n",
    "**Visualizations created:**\n",
    "1. Cluster size distribution (bar chart)\n",
    "2. ROI comparison across clusters (box plot)\n",
    "3. Trading activity heatmap (trade frequency vs holding days)\n",
    "4. Narrative exposure comparison (stacked bar chart)\n",
    "\n",
    "**Expected output:** 4 matplotlib figures showing cluster characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "non_noise_clusters = [c for c in hdbscan_clusters if c != -1]\n",
    "cluster_names_short = {c: personas[c]['name'][:20] for c in hdbscan_clusters}\n",
    "\n",
    "# Viz 1: Cluster Size Distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sizes = [cluster_profiles[c]['size'] for c in hdbscan_clusters]\n",
    "labels = [f\"Cluster {c}\\n{cluster_names_short[c]}\" if c != -1 else \"Noise\\nUnique\" \n",
    "          for c in hdbscan_clusters]\n",
    "colors = ['red' if c == -1 else 'steelblue' for c in hdbscan_clusters]\n",
    "\n",
    "bars = ax.bar(range(len(sizes)), sizes, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(len(sizes)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Number of Wallets', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Cluster Size Distribution (HDBSCAN Optimized)', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, size) in enumerate(zip(bars, sizes)):\n",
    "    height = bar.get_height()\n",
    "    pct = size / len(df) * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
    "            f'{size:,}\\n({pct:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Cluster size distribution visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz 2: ROI Distribution by Cluster (Box Plot)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for box plot\n",
    "roi_data = []\n",
    "labels_clean = []\n",
    "for c in hdbscan_clusters:\n",
    "    cluster_data = df[df['hdbscan_cluster'] == c]['roi_percent']\n",
    "    roi_data.append(cluster_data)\n",
    "    label = \"Noise\" if c == -1 else f\"C{c}\"\n",
    "    labels_clean.append(label)\n",
    "\n",
    "bp = ax.boxplot(roi_data, labels=labels_clean, patch_artist=True,\n",
    "                showfliers=True, notch=True)\n",
    "\n",
    "# Color boxes\n",
    "for i, (patch, c) in enumerate(zip(bp['boxes'], hdbscan_clusters)):\n",
    "    color = 'lightcoral' if c == -1 else 'lightblue'\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('ROI %', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROI Distribution by Cluster', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Break-even')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ ROI distribution visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz 3: Trading Activity Heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Prepare data\n",
    "activity_data = []\n",
    "for c in non_noise_clusters:\n",
    "    activity_data.append([\n",
    "        cluster_profiles[c]['trade_freq_mean'],\n",
    "        cluster_profiles[c]['holding_days_mean'],\n",
    "        cluster_profiles[c]['weekend_ratio'] * 100,\n",
    "        cluster_profiles[c]['night_ratio'] * 100,\n",
    "    ])\n",
    "\n",
    "activity_df = pd.DataFrame(\n",
    "    activity_data,\n",
    "    columns=['Trade Frequency', 'Holding Days', 'Weekend Activity %', 'Night Trading %'],\n",
    "    index=[f\"Cluster {c}\" for c in non_noise_clusters]\n",
    ")\n",
    "\n",
    "# Normalize for heatmap\n",
    "activity_norm = activity_df.copy()\n",
    "for col in activity_norm.columns:\n",
    "    min_val = activity_norm[col].min()\n",
    "    max_val = activity_norm[col].max()\n",
    "    if max_val > min_val:\n",
    "        activity_norm[col] = (activity_norm[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "sns.heatmap(activity_norm.T, annot=activity_df.T.values, fmt='.1f',\n",
    "            cmap='YlOrRd', cbar_kws={'label': 'Normalized Value'},\n",
    "            linewidths=0.5, linecolor='gray', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Activity Metric', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Trading Activity Profile by Cluster', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Trading activity heatmap created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz 4: Narrative Exposure Comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Prepare data\n",
    "narratives = ['defi_exposure', 'ai_exposure', 'meme_exposure']\n",
    "narrative_labels = ['DeFi', 'AI', 'Meme']\n",
    "cluster_labels = [f\"C{c}\" for c in non_noise_clusters]\n",
    "\n",
    "narrative_data = {\n",
    "    label: [cluster_profiles[c][narrative] for c in non_noise_clusters]\n",
    "    for label, narrative in zip(narrative_labels, narratives)\n",
    "}\n",
    "\n",
    "x = np.arange(len(cluster_labels))\n",
    "width = 0.25\n",
    "\n",
    "for i, (label, values) in enumerate(narrative_data.items()):\n",
    "    offset = (i - 1) * width\n",
    "    ax.bar(x + offset, values, width, label=label, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Exposure %', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Narrative Exposure by Cluster', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cluster_labels)\n",
    "ax.legend(title='Narrative', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Narrative exposure comparison created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Export Results\n",
    "\n",
    "**What we're doing:** Save all analysis results to structured files for documentation and further use.\n",
    "\n",
    "**Why:** Exportable data enables:\n",
    "- Integration with other analyses\n",
    "- Sharing with stakeholders\n",
    "- Reproducibility\n",
    "- Version control of findings\n",
    "\n",
    "**Files exported:**\n",
    "1. Cluster profiles (CSV) - detailed statistics\n",
    "2. Cluster personas (JSON) - rich narratives\n",
    "3. Cluster insights (JSON) - actionable recommendations\n",
    "4. Representative wallets (JSON) - example addresses\n",
    "5. HDBSCAN/K-Means comparison (CSV) - validation\n",
    "6. Cluster overlap analysis (CSV) - quantified agreement\n",
    "7. Feature validation report (TXT) - data quality issues\n",
    "\n",
    "**Expected output:** 7 files saved to `/outputs/cluster_interpretation/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"Exporting analysis results...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export 1: Cluster Profiles\n",
    "profiles_export = []\n",
    "for cluster_id, profile in cluster_profiles.items():\n",
    "    profile_export = profile.copy()\n",
    "    profile_export['cluster_name'] = personas[cluster_id]['name']\n",
    "    profile_export['archetype'] = personas[cluster_id]['archetype']\n",
    "    profiles_export.append(profile_export)\n",
    "\n",
    "profiles_df = pd.DataFrame(profiles_export)\n",
    "profiles_file = OUTPUT_DIR / f\"cluster_profiles_detailed_{timestamp}.csv\"\n",
    "profiles_df.to_csv(profiles_file, index=False)\n",
    "print(f\"✅ {profiles_file.name}\")\n",
    "\n",
    "# Export 2: Cluster Personas\n",
    "personas_file = OUTPUT_DIR / f\"cluster_personas_{timestamp}.json\"\n",
    "with open(personas_file, 'w') as f:\n",
    "    personas_export = {str(k): v for k, v in personas.items()}\n",
    "    json.dump(personas_export, f, indent=2)\n",
    "print(f\"✅ {personas_file.name}\")\n",
    "\n",
    "# Export 3: Cluster Insights\n",
    "insights_file = OUTPUT_DIR / f\"cluster_insights_{timestamp}.json\"\n",
    "with open(insights_file, 'w') as f:\n",
    "    insights_export = {str(k): v for k, v in cluster_insights.items()}\n",
    "    json.dump(insights_export, f, indent=2)\n",
    "print(f\"✅ {insights_file.name}\")\n",
    "\n",
    "# Export 4: Representative Wallets\n",
    "rep_wallets_file = OUTPUT_DIR / f\"representative_wallets_{timestamp}.json\"\n",
    "with open(rep_wallets_file, 'w') as f:\n",
    "    rep_export = {str(k): v for k, v in representative_wallets.items()}\n",
    "    json.dump(rep_export, f, indent=2)\n",
    "print(f\"✅ {rep_wallets_file.name}\")\n",
    "\n",
    "# Export 5: Cross-tabulation\n",
    "comparison_file = OUTPUT_DIR / f\"hdbscan_kmeans_comparison_{timestamp}.csv\"\n",
    "cross_tab.to_csv(comparison_file)\n",
    "print(f\"✅ {comparison_file.name}\")\n",
    "\n",
    "# Export 6: Overlap Analysis\n",
    "overlap_file = OUTPUT_DIR / f\"cluster_overlap_analysis_{timestamp}.csv\"\n",
    "overlap_df.to_csv(overlap_file, index=False)\n",
    "print(f\"✅ {overlap_file.name}\")\n",
    "\n",
    "# Export 7: Validation Report\n",
    "validation_file = OUTPUT_DIR / f\"feature_validation_report_{timestamp}.txt\"\n",
    "with open(validation_file, 'w') as f:\n",
    "    f.write(\"FEATURE VALIDATION REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    failed = [r for r in validation_results if r['status'] == 'fail']\n",
    "    if failed:\n",
    "        f.write(f\"Found {len(failed)} validation issue(s):\\n\\n\")\n",
    "        for issue in failed:\n",
    "            f.write(f\"⚠️  {issue['feature']}: \")\n",
    "            f.write(f\"Actual {issue['actual_range']} vs Expected {issue['expected_range']}\\n\")\n",
    "        f.write(\"\\nRecommendation: Review feature engineering logic for affected features.\\n\")\n",
    "    else:\n",
    "        f.write(\"✅ All features validated successfully\\n\")\n",
    "\n",
    "print(f\"✅ {validation_file.name}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ All results exported to: {OUTPUT_DIR}\")\n",
    "print(f\"   Total files: 7\")\n",
    "print(f\"   Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Findings\n",
    "\n",
    "**Analysis Complete!**\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Large Noise Cluster (48.4%)**\n",
    "   - Nearly half of all wallets have unique strategies\n",
    "   - Higher variance than clustered wallets\n",
    "   - Contains exceptional performers (up to 258% ROI)\n",
    "   - Research insight: Crypto markets reward heterogeneous strategies\n",
    "\n",
    "2. **Homogeneous Cluster Characteristics**\n",
    "   - 13 non-noise clusters share similar profiles\n",
    "   - ROI centered around 79.4%\n",
    "   - Highly concentrated portfolios (HHI > 7,500)\n",
    "   - Passive trading (1-2 trades average)\n",
    "\n",
    "3. **Strong Algorithm Validation**\n",
    "   - 90-100% overlap between HDBSCAN and K-Means\n",
    "   - 7 clusters with perfect 100% agreement\n",
    "   - Validates clustering quality despite moderate silhouette scores\n",
    "\n",
    "4. **Data Quality Issue**\n",
    "   - portfolio_hhi using 0-10,000 scale instead of 0-1\n",
    "   - Documented for future refinement\n",
    "   - Doesn't invalidate current analysis\n",
    "\n",
    "### Actionable Recommendations:\n",
    "\n",
    "**For Researchers:**\n",
    "- Focus on noise cluster for unique strategy discovery\n",
    "- Implement temporal clustering (monthly cohorts)\n",
    "- Fix feature engineering issues (HHI scaling, win_rate)\n",
    "\n",
    "**For Traders:**\n",
    "- Successful Tier 1 wallets use concentrated portfolios\n",
    "- Passive trading (1-2 strategic entries) is common\n",
    "- Target ~80% ROI as benchmark\n",
    "\n",
    "**For Developers:**\n",
    "- 2 primary segments: Conforming (51.6%) vs Unique (48.4%)\n",
    "- Tailor features/UX for each segment\n",
    "- Traditional risk metrics may not apply to crypto\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Review exported files in `/outputs/cluster_interpretation/`\n",
    "2. Use representative wallets for case study deep-dives\n",
    "3. Implement temporal clustering to study strategy evolution\n",
    "4. Fix feature engineering issues and re-run clustering\n",
    "\n",
    "---\n",
    "\n",
    "**Story 4.4 Complete!** ✅"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
