# Comprehensive EDA Validation Report
## Wallet Features Master Dataset

**Dataset:** `wallet_features_master_20251022_195455.csv`
**Analysis Date:** October 25, 2025
**Analyst:** Blockchain Data Analysis System

---

## Executive Summary

### Dataset Overview
- **Size:** 2,159 wallets Ã— 34 features
- **Memory Usage:** 0.75 MB
- **Data Completeness:** 100% (0 missing values)
- **Unique Wallets:** 2,159 (all unique, no duplicates)

### Overall Assessment

**Data Quality Score: 90.8/100 (EXCELLENT)**

| Dimension | Score | Assessment |
|-----------|-------|------------|
| Completeness | 100/100 | No missing values |
| Validity | 95/100 | Some out-of-range values in 3 features |
| Accuracy | 60/100 | Extreme value errors in PnL/trade size |
| Consistency | 90/100 | Addresses valid, no duplicates |
| Uniqueness | 100/100 | All wallet addresses unique |
| Timeliness | 100/100 | Recent data (Oct 2025) |

**ML Readiness Status:** âŒ **NOT READY** - Critical issues must be fixed first

---

## Key Findings

### 1. Excellent Data Quality - Structure

âœ… **Strengths:**
- All 2,159 wallet addresses are valid Ethereum addresses (0x + 40 hexadecimal characters)
- Zero missing values across all 34 features
- No duplicate records
- Perfect structural integrity
- All address lengths exactly 42 characters
- No null values in identifier columns

### 2. Critical Data Quality Issues

#### ðŸ”´ Zero-Variance Features (MUST REMOVE)

**Problem:** Features with constant values provide no information for ML models

| Feature | Value | Impact |
|---------|-------|--------|
| `gas_optimization_score` | 50.0 (100% constant) | Zero predictive power |
| `dex_diversity_score` | 0.0 (100% constant) | Zero predictive power |

**Action Required:** Remove both features immediately

#### ðŸ”´ Out-of-Range Values

**Problem:** Values violating expected constraints

| Feature | Expected Range | Actual Range | Issues |
|---------|---------------|--------------|--------|
| `roi_percent` | [0, 100] or [-100, âˆž] | [-99.99, 258.44] | 10 outliers (0.46%) |
| `volume_consistency` | [0, 1] (ratio) | [0, 2.41] | 26 violations (1.20%) |
| `top3_concentration_pct` | [0, 100] | [0, 100] | 9 edge cases near boundaries |

**Blockchain Context:**
ROI values >100% are possible (258% ROI indicates 2.58x returns), but negative ROI near -100% suggests complete loss. These outliers are legitimate but rare cases.

#### ðŸ”´ Extreme Value Errors

**Problem:** Mathematically impossible values indicating calculation errors

```
total_pnl_usd:
  Min: -$61,785,415,625,965,412,352.00 (-$61.8 QUINTILLION)
  Max: $71,905,809,502.58 ($71.9 BILLION)

avg_trade_size_usd:
  Min: $0.00000000000000001747
  Max: $671,580,600,000,000,000.00 ($671.6 QUADRILLION)
```

**Evidence of Error:**
- Perfect negative correlation: `corr(total_pnl_usd, avg_trade_size_usd) = -1.000`
- These features appear to be mathematical inverses with overflow errors
- 93.28% of wallets have exactly zero PnL

**Action Required:**
1. Investigate calculation logic in feature engineering pipeline
2. Fix overflow/underflow issues
3. Recalculate or cap at reasonable bounds
4. Consider removing one feature (they're perfectly redundant)

#### ðŸ”´ Perfect Multicollinearity

**Problem:** Redundant features that provide identical information

| Feature Pair | Correlation | Recommendation |
|--------------|-------------|----------------|
| `avg_holding_period_days` â†” `diamond_hands_score` | r = 1.000 | Keep `avg_holding_period_days`, remove `diamond_hands_score` |
| `total_pnl_usd` â†” `avg_trade_size_usd` | r = -1.000 | Fix errors, then keep one |

**ML Impact:**
Perfect multicollinearity inflates model variance, makes coefficients uninterpretable, and can cause numerical instability in some algorithms.

### 3. High Data Sparsity

**Problem:** 20 features have >50% zero values, indicating sparse data

| Feature | Zero Count | % Zeros | Interpretation |
|---------|-----------|---------|----------------|
| `dex_diversity_score` | 2,159 | 100.00% | All wallets trade on single DEX |
| `rotation_frequency` | 2,144 | 99.31% | Almost no portfolio rotation |
| `num_tokens_std` | 2,154 | 99.77% | Portfolio size very stable |
| `portfolio_turnover` | 2,144 | 99.31% | Minimal buying/selling activity |
| `ai_exposure_pct` | 2,052 | 95.04% | Very few AI token holders |
| `win_rate` | 2,039 | 94.44% | Most wallets have no winning trades |
| `total_pnl_usd` | 2,014 | 93.28% | No realized P&L |
| `meme_exposure_pct` | 1,885 | 87.31% | Limited meme token exposure |
| `defi_exposure_pct` | 1,752 | 81.15% | Limited DeFi protocol usage |
| `stablecoin_usage_ratio` | 1,690 | 78.28% | Most don't use stablecoins |

**Blockchain Context:**
This sparsity pattern suggests the dataset contains many **passive holders** rather than active traders. This is common in blockchain data where most addresses have minimal activity.

**ML Implications:**
- Sparse features often have low predictive power
- Can cause overfitting if not handled properly
- May need special treatment (binary flags, separate models for active vs inactive)

### 4. Distribution Characteristics

#### Highly Skewed Features

| Feature | Skewness | Kurtosis | Distribution Type |
|---------|----------|----------|-------------------|
| `trade_frequency` | 27.79 | 948.15 | Extreme right-skew (power law) |
| `max_drawdown_pct` | 14.34 | 299.38 | Heavy right tail |
| `total_pnl_usd` | -46.47 | 2159.00 | Extreme left-skew (errors) |
| `avg_trade_size_usd` | 46.47 | 2159.00 | Extreme right-skew (errors) |
| `top3_concentration_pct` | -8.07 | 70.03 | Heavy left tail |
| `roi_percent` | -1.58 | 103.72 | Left-skewed, heavy tails |
| `sharpe_ratio` | -8.22 | 73.02 | Extreme left-skew |

**Blockchain Insight:**
Power-law distributions are expected in blockchain data:
- Transaction values: Many small, few large (Pareto principle)
- Wallet activity: Most inactive, few hyperactive
- Token holdings: High inequality (Gini coefficients confirm)

**Action Required:**
Apply log transformations to extremely skewed features before modeling

#### Single-Trade Wallet Dominance

```
Trade Frequency Distribution:
  Single trade (frequency = 1): 1,659 wallets (76.84%)
  2-10 trades: 456 wallets (21.13%)
  >10 trades: 44 wallets (2.04%)

  Max trade frequency: 394 trades
  Median: 1.0 trades
  Mean: 1.93 trades
```

**Implication:**
The dataset is heavily skewed toward **one-time users**. Consider:
- Filtering to active traders only (trade_frequency > 1)
- Separate models for inactive vs active wallets
- Stratified sampling to balance classes

### 5. Multicollinearity Analysis

#### High Correlations (|r| > 0.7)

| Feature 1 | Feature 2 | Correlation | Explanation |
|-----------|-----------|-------------|-------------|
| `roi_percent` | `sharpe_ratio` | 0.891 | Both measure performance |
| `sharpe_ratio` | `top3_concentration_pct` | 0.759 | Concentrated portfolios = higher Sharpe |
| `max_drawdown_pct` | `distribution_intensity` | 0.895 | Drawdown occurs during distribution |
| `total_pnl_usd` | `avg_trade_size_usd` | -1.000 | Mathematical inverse (error) |
| `avg_holding_period_days` | `diamond_hands_score` | 1.000 | Same metric, different name |

#### Moderate Correlations (0.5 < |r| â‰¤ 0.7)

| Feature 1 | Feature 2 | Correlation | Interpretation |
|-----------|-----------|-------------|----------------|
| `roi_percent` | `trend_direction` | 0.697 | Positive trends â†’ positive returns |
| `portfolio_hhi` | `primary_narrative_pct` | 0.620 | Concentrated portfolios follow single narrative |
| `narrative_diversity_score` | `primary_narrative_pct` | -0.675 | By definition (inverse relationship) |
| `accumulation_intensity` | `trend_direction` | 0.615 | Accumulation correlates with uptrends |
| `distribution_intensity` | `balance_volatility` | 0.698 | Distribution causes volatility |

**ML Impact:**
Moderate correlations suggest thematic groupings. Consider feature selection or PCA to reduce dimensionality.

### 6. Behavioral Insights

#### Portfolio Concentration
```
Portfolio Metrics:
  Median HHI: 9,978.28 (near maximum 10,000)
  Median Gini: 0.665 (moderate-high inequality)
  Median Top3 Concentration: 100%

Interpretation: Most wallets hold 1-3 tokens with one dominant holding
```

**Blockchain Context:**
High concentration is typical for:
- New wallets focused on single project
- Speculative traders in specific narratives
- Not diversified portfolio management

#### Narrative Exposure

```
Exposure Distribution (among wallets with exposure):
  DeFi: 407 wallets (18.85%) | Mean exposure: 45.58%
  AI: 107 wallets (4.96%) | Mean exposure: 35.30%
  Meme: 274 wallets (12.69%) | Mean exposure: 36.51%

Primary Narrative Dominance:
  Mean: 95.56% (wallets heavily commit to one narrative)
```

**Insight:**
Wallets tend to be **narrative specialists** rather than diversified across themes.

---

## Detailed Data Quality Issues

### Column-by-Column Analysis

#### Performance Metrics

**1. roi_percent**
- Range: [-99.99%, 258.44%]
- Mean: 78.88% | Median: 79.44%
- Issues: 10 outliers outside typical range
- Distribution: Left-skewed with heavy tails
- **Action:** Keep as-is; outliers are legitimate extreme performers

**2. win_rate**
- Range: [0%, 100%]
- Mean: 3.27% | Median: 0%
- Issues: 94.44% have zero (no winning trades)
- Distribution: Extreme right-skew
- **Action:** Useful signal for non-zero cases; consider binary "has_wins" flag

**3. sharpe_ratio**
- Range: [-0.55, 5.08]
- Mean: 3.46 | Median: 3.50
- Issues: 888 outliers (41% of data)
- Distribution: Left-skewed, heavy tails
- **Action:** High correlation (0.89) with ROI; consider removing one

**4. max_drawdown_pct**
- Range: [0%, 100%]
- Mean: 16.69% | Median: 16.63%
- Issues: Extreme values (100% drawdown = total loss)
- Distribution: Right-skewed, heavy tails
- **Action:** Valid metric; consider capping at 95th percentile for modeling

**5. total_pnl_usd**
- Range: [-$61.8 quintillion, $71.9 billion]
- Mean: -$2.86e16 (clearly wrong)
- Issues: **CRITICAL - Calculation error**
- **Action:** Must fix before modeling

#### Trading Behavior Metrics

**6. avg_trade_size_usd**
- Range: [$0 (essentially), $671.6 quadrillion]
- Issues: **CRITICAL - Calculation error** (perfect inverse of PnL)
- **Action:** Fix calculation or remove

**7. volume_consistency**
- Range: [0, 2.41]
- Expected: [0, 1]
- Issues: 26 values > 1.0
- **Action:** Investigate calculation; clip to [0, 1] or redefine metric

**8. trade_frequency**
- Range: [1, 394]
- Mean: 1.93 | Median: 1.0
- Issues: Extreme right-skew (76.84% = 1)
- **Action:** Apply log transformation; consider filtering inactive wallets

**9. avg_holding_period_days**
- Range: [0, 24.96 days]
- Mean: 2.76 | Median: 0
- Issues: 51.37% have zero (single-block trades)
- **Action:** Keep; useful for active traders

**10. diamond_hands_score**
- **ISSUE:** Perfect correlation (r=1.0) with holding period
- **Action:** REMOVE (redundant)

**11. rotation_frequency**
- Range: [0, 0.47]
- Issues: 99.31% are zero
- **Action:** Consider removing (too sparse) or convert to binary flag

#### Temporal & Activity Patterns

**12. weekend_activity_ratio**
- Range: [0, 1]
- Mean: 0.26 | Median: 0
- Issues: 57.71% are zero
- **Action:** Keep; useful behavioral signal when non-zero

**13. night_trading_ratio**
- Range: [0, 1]
- Mean: 0.43 | Median: 0.33
- Issues: Less sparse than weekend activity
- **Action:** Keep; good behavioral indicator

**14. gas_optimization_score**
- **ISSUE:** 100% constant (value = 50.0)
- **Action:** REMOVE immediately (zero variance)

**15. dex_diversity_score**
- **ISSUE:** 100% constant (value = 0.0)
- Interpretation: All wallets use single DEX
- **Action:** REMOVE immediately (zero variance)

#### Portfolio Concentration Metrics

**16. portfolio_hhi**
- Range: [0, 10,000]
- Mean: 8,661.50 | Median: 9,978.28
- Interpretation: High concentration (HHI near 10,000 = monopoly)
- **Action:** Keep; valid concentration measure

**17. portfolio_gini**
- Range: [0, 0.99]
- Mean: 0.635 | Median: 0.665
- Interpretation: Moderate-high inequality
- **Action:** Keep; complements HHI

**18. top3_concentration_pct**
- Range: [0%, 100%]
- Mean: 98.12% | Median: 100%
- Issues: 9 edge cases; high correlation (0.76) with Sharpe
- **Action:** Keep but monitor for collinearity

**19. num_tokens_avg**
- Range: [1, 100]
- Mean: 26.43 | Median: 6.0
- Distribution: Right-skewed
- **Action:** Keep; useful portfolio size indicator

**20. num_tokens_std**
- Range: [0, 0.51]
- Issues: 99.77% are zero (portfolios don't change size)
- **Action:** Remove (too sparse, no signal)

**21. portfolio_turnover**
- Range: [0, 0.013]
- Issues: 99.31% are zero
- **Action:** Remove (too sparse) or convert to binary flag

#### Narrative Exposure Metrics

**22. narrative_diversity_score**
- Range: [0, 1.84]
- Mean: 0.12 | Median: 0.000042
- Distribution: Highly right-skewed
- **Action:** Keep; measures cross-narrative exposure

**23. primary_narrative_pct**
- Range: [0%, 100%]
- Mean: 95.56% | Median: 100%
- Interpretation: Wallets commit heavily to single narrative
- **Action:** Keep; strong signal of specialization

**24-26. Specific Narrative Exposures (defi_exposure_pct, ai_exposure_pct, meme_exposure_pct)**
- Issues: All highly sparse (81-95% zeros)
- **Action:** Keep for narrative-specific models; consider binary flags for general models

**27. stablecoin_usage_ratio**
- Range: [0, 1]
- Mean: 0.22 | Median: 0
- Issues: 78.28% are zero
- **Action:** Keep; useful risk management indicator

#### Balance & Trend Metrics

**28-29. accumulation_phase_days / distribution_phase_days**
- Issues: 68% and 72% zeros respectively
- Correlation: 0.63 (related but distinct)
- **Action:** Keep for active traders; consider filtering or binary flags

**30-31. accumulation_intensity / distribution_intensity**
- Issues: Similar sparsity to phase days
- High correlation: max_drawdown â†” distribution_intensity (r=0.89)
- **Action:** Keep one from each correlated pair

**32. balance_volatility**
- Range: [0, 70.71]
- Mean: 0.21 | Median: 0
- Issues: 59.61% are zero
- **Action:** Keep; useful for active wallet risk profiling

**33. trend_direction**
- Range: [-0.76, 0.76]
- Mean: 0.001 | Median: 0
- Issues: 59.75% are zero
- High correlation: roi_percent (r=0.70)
- **Action:** Keep; captures momentum behavior

---

## Outlier Analysis

### IQR-Based Outlier Detection

Features with >20% outliers (IQR method with multiplier=1.5):

| Feature | Outlier Count | % of Data | Explanation |
|---------|--------------|-----------|-------------|
| `roi_percent` | 893 | 41.36% | Tight clustering around ~79.44% |
| `sharpe_ratio` | 888 | 41.13% | Tight clustering around ~3.50 |
| `trade_frequency` | 500 | 23.16% | Single-trade dominance creates outliers |
| `accumulation_intensity` | 500 | 23.16% | Sparsity creates artificial outliers |
| `distribution_intensity` | 525 | 24.32% | Sparsity creates artificial outliers |

**Interpretation:**
High outlier percentages don't necessarily indicate data quality issues. They reflect:
1. **Natural clustering** in blockchain data (most wallets behave similarly)
2. **True outliers** (whale traders, sophisticated strategies)
3. **Sparsity effects** (many zeros make non-zeros appear as outliers)

### Z-Score Outlier Detection (threshold = 3)

Features with extreme outliers (Z-score > 3):

| Feature | Outlier Count | % of Data | Max Z-Score |
|---------|--------------|-----------|-------------|
| `total_pnl_usd` | 1 | 0.05% | **46.44** (data error) |
| `avg_trade_size_usd` | 1 | 0.05% | **46.44** (data error) |
| `trade_frequency` | 10 | 0.46% | **37.43** (legitimate hyperactive traders) |
| `balance_volatility` | 8 | 0.37% | **36.03** (legitimate high volatility) |
| `accumulation_intensity` | 9 | 0.42% | **27.73** (aggressive accumulators) |
| `distribution_intensity` | 8 | 0.37% | **24.04** (aggressive sellers) |

**Action:**
- **Fix:** PnL and trade size errors (Z-score 46 is impossible)
- **Keep:** Other outliers are legitimate extreme behaviors in blockchain trading

---

## Feature Engineering Recommendations

### Immediate Actions (Critical)

1. **Remove Zero-Variance Features**
   ```python
   features_to_remove = [
       'gas_optimization_score',  # constant = 50.0
       'dex_diversity_score',     # constant = 0.0
   ]
   ```

2. **Remove Redundant Features**
   ```python
   redundant_features = [
       'diamond_hands_score',  # r=1.0 with avg_holding_period_days
       # Keep one: total_pnl_usd OR avg_trade_size_usd (after fixing)
   ]
   ```

3. **Fix Calculation Errors**
   ```python
   # Investigate and fix:
   # - total_pnl_usd (extreme negative values)
   # - avg_trade_size_usd (extreme positive values)
   # - volume_consistency (values > 1.0)
   ```

### Transform Highly Skewed Features

```python
# Log transformations for power-law distributions
features_to_log_transform = [
    'trade_frequency',
    'num_tokens_avg',
    'balance_volatility',
    'accumulation_intensity',
    'distribution_intensity'
]

# Apply: log1p (log(x + 1)) to handle zeros
df[f'{feature}_log'] = np.log1p(df[feature])
```

### Create Binary Indicators for Sparse Features

```python
# Convert sparse continuous features to binary flags
binary_conversions = {
    'has_win_rate': 'win_rate > 0',
    'has_weekend_activity': 'weekend_activity_ratio > 0',
    'has_defi_exposure': 'defi_exposure_pct > 0',
    'has_ai_exposure': 'ai_exposure_pct > 0',
    'has_meme_exposure': 'meme_exposure_pct > 0',
    'uses_stablecoins': 'stablecoin_usage_ratio > 0',
    'has_rotation': 'rotation_frequency > 0',
    'has_turnover': 'portfolio_turnover > 0',
}
```

### Create Interaction Features

```python
# Economically meaningful combinations
interaction_features = {
    'roi_per_trade': 'roi_percent / trade_frequency',
    'risk_adjusted_return': 'roi_percent / (max_drawdown_pct + 1)',
    'portfolio_complexity': 'num_tokens_avg * narrative_diversity_score',
    'concentration_gini_product': 'portfolio_hhi * portfolio_gini',
    'active_trader_score': 'trade_frequency * volume_consistency',
}
```

### Create Wallet Segments

```python
# Activity-based segmentation
def segment_wallet(row):
    if row['trade_frequency'] == 1:
        return 'inactive'
    elif row['trade_frequency'] <= 10:
        return 'casual'
    else:
        return 'active'

# Performance-based segmentation
def performance_segment(row):
    if row['roi_percent'] < 0:
        return 'loser'
    elif row['roi_percent'] < 50:
        return 'modest_winner'
    elif row['roi_percent'] < 100:
        return 'strong_winner'
    else:
        return 'exceptional'
```

---

## ML Modeling Recommendations

### Data Preprocessing Pipeline

```python
# Recommended preprocessing order:
# 1. Remove zero-variance and redundant features
# 2. Fix calculation errors in PnL/trade size
# 3. Handle outliers (cap at 99th percentile or use robust scalers)
# 4. Apply log transformations to skewed features
# 5. Create binary indicators for sparse features
# 6. Create interaction features
# 7. Scale/normalize remaining features
# 8. Check for remaining multicollinearity (VIF < 10)
```

### Train/Val/Test Split Strategy

**âŒ DON'T:** Random split (causes data leakage if wallets have temporal structure)

**âœ… DO:** Stratified split on key variables

```python
from sklearn.model_selection import train_test_split

# Option 1: Stratify by wallet activity
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.3,
    stratify=df['wallet_segment'],  # inactive/casual/active
    random_state=42
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5,
    stratify=y_temp,
    random_state=42
)

# Split: 70% train, 15% validation, 15% test
```

### Handling Class Imbalance

**Problem:** 76.84% single-trade wallets dominate the dataset

**Solutions:**

1. **Filter to Active Traders Only**
   ```python
   active_wallets = df[df['trade_frequency'] > 1]
   # Reduces dataset to ~500 wallets but improves signal
   ```

2. **Stratified Sampling**
   ```python
   # Oversample minority class (active traders)
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
   ```

3. **Class Weights**
   ```python
   from sklearn.utils.class_weight import compute_class_weight
   weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
   # Use in model: model.fit(X, y, class_weight=weights)
   ```

4. **Ensemble of Specialized Models**
   - Model 1: Inactive wallets (trade_freq = 1)
   - Model 2: Casual traders (trade_freq 2-10)
   - Model 3: Active traders (trade_freq > 10)

### Feature Selection Strategy

**Step 1: Remove low-value features**
```python
features_to_remove = [
    'gas_optimization_score',     # zero variance
    'dex_diversity_score',        # zero variance
    'diamond_hands_score',        # redundant
    'num_tokens_std',             # 99.77% zeros
    'rotation_frequency',         # 99.31% zeros
    'portfolio_turnover',         # 99.31% zeros
]
```

**Step 2: Test multicollinearity**
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

# Remove features with VIF > 10
high_vif = vif_data[vif_data['VIF'] > 10]
```

**Step 3: Feature importance from tree models**
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importances
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# Keep top N features
top_features = importances.head(20)['feature'].tolist()
```

### Recommended Model Types

Based on data characteristics, these models are well-suited:

1. **Gradient Boosting (XGBoost, LightGBM, CatBoost)**
   - âœ… Handles non-linear relationships
   - âœ… Robust to outliers
   - âœ… Handles missing values (if created during transforms)
   - âœ… Feature importance built-in
   - âœ… Performs well with imbalanced classes

2. **Random Forest**
   - âœ… Robust to outliers
   - âœ… Handles mixed feature types
   - âœ… Feature importance available
   - âŒ May overfit with default hyperparameters

3. **Regularized Linear Models (Lasso, ElasticNet)**
   - âœ… Automatic feature selection
   - âœ… Interpretable coefficients
   - âŒ Requires careful scaling
   - âŒ Assumes linear relationships

4. **Neural Networks (if sufficient data after filtering)**
   - âœ… Can learn complex interactions
   - âœ… Good for high-dimensional data
   - âŒ Requires careful regularization
   - âŒ Less interpretable

**Recommendation:** Start with **LightGBM** or **XGBoost**
- Handles skewed distributions well
- Built-in handling of sparse features
- Fast training
- Good performance on tabular data

### Evaluation Metrics

**For Classification (e.g., predicting successful wallets):**

```python
from sklearn.metrics import (
    roc_auc_score,
    precision_recall_curve,
    average_precision_score,
    classification_report
)

# Primary metric (for imbalanced classes):
pr_auc = average_precision_score(y_true, y_pred_proba)

# Secondary metrics:
roc_auc = roc_auc_score(y_true, y_pred_proba)
print(classification_report(y_true, y_pred))
```

**For Regression (e.g., predicting ROI):**

```python
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score
)

mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2 = r2_score(y_true, y_pred)

# Also consider:
# - MAPE (Mean Absolute Percentage Error) for relative performance
# - Huber loss for robustness to outliers
```

### Cross-Validation Strategy

```python
from sklearn.model_selection import StratifiedKFold

# Use stratified K-fold to maintain class distributions
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = []
for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    X_train_fold = X[train_idx]
    X_val_fold = X[val_idx]
    y_train_fold = y[train_idx]
    y_val_fold = y[val_idx]

    # Train model
    model.fit(X_train_fold, y_train_fold)

    # Evaluate
    score = model.score(X_val_fold, y_val_fold)
    scores.append(score)

print(f"CV Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})")
```

---

## Blockchain-Specific Considerations

### Network Effects

**Insight:** Wallets don't trade in isolation. Consider:

1. **Address clustering** - Identify related addresses (likely same entity)
2. **Contract interaction patterns** - Wallets interacting with same contracts
3. **Temporal clustering** - Coordinated trading patterns

### Economic Incentives

**Context matters:**
- Gas prices affect trade timing and size
- MEV (Maximal Extractable Value) influences sophisticated traders
- Protocol incentives (airdrops, liquidity mining) drive behavior

### Data Stationarity

**Warning:** Blockchain markets are non-stationary
- Bull/bear market regimes have different patterns
- Protocol upgrades change behavior
- Narrative shifts alter trading patterns

**Recommendation:**
- Add regime indicators (market phase, volatility regime)
- Retrain models periodically
- Monitor for distribution drift

### Sybil Resistance

**Concern:** Multiple addresses controlled by single entity

**Detection strategies:**
- Similar transaction patterns (timing, amounts, tokens)
- Shared funding sources
- Coordinated behavior

**Impact on ML:**
- Overfitting to single entity behavior
- Biased performance estimates

---

## Action Plan: Path to ML-Ready Dataset

### Phase 1: Critical Fixes (Required)

**Estimated Time: 2-3 hours**

```python
# 1. Remove zero-variance features
df_clean = df.drop(columns=['gas_optimization_score', 'dex_diversity_score'])

# 2. Remove redundant features
df_clean = df_clean.drop(columns=['diamond_hands_score'])

# 3. Fix calculation errors (requires investigation of source code)
# TODO: Fix total_pnl_usd and avg_trade_size_usd calculation
# For now, remove or cap extreme values:
df_clean['total_pnl_usd'] = df_clean['total_pnl_usd'].clip(
    lower=-1e12,  # -$1 trillion (extreme but plausible loss)
    upper=1e12    # $1 trillion (extreme but plausible gain)
)

# 4. Fix out-of-range values
df_clean['volume_consistency'] = df_clean['volume_consistency'].clip(0, 1)
df_clean['roi_percent'] = df_clean['roi_percent'].clip(-100, 500)  # Allow up to 5x returns
```

### Phase 2: Feature Engineering (Important)

**Estimated Time: 1-2 hours**

```python
# 1. Log transformations
skewed_features = ['trade_frequency', 'num_tokens_avg', 'balance_volatility']
for feat in skewed_features:
    df_clean[f'{feat}_log'] = np.log1p(df_clean[feat])

# 2. Create binary indicators
df_clean['is_active'] = (df_clean['trade_frequency'] > 1).astype(int)
df_clean['has_defi_exposure'] = (df_clean['defi_exposure_pct'] > 0).astype(int)
df_clean['has_wins'] = (df_clean['win_rate'] > 0).astype(int)

# 3. Create interaction features
df_clean['roi_per_trade'] = df_clean['roi_percent'] / (df_clean['trade_frequency'] + 1)
df_clean['risk_adjusted_return'] = df_clean['roi_percent'] / (df_clean['max_drawdown_pct'] + 1)

# 4. Create segments
df_clean['activity_segment'] = pd.cut(
    df_clean['trade_frequency'],
    bins=[0, 1, 10, 1000],
    labels=['inactive', 'casual', 'active']
)
```

### Phase 3: Data Splitting & Validation (Essential)

**Estimated Time: 30 minutes**

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Split data
X = df_clean.drop(columns=['wallet_address', 'target_variable'])  # Define target
y = df_clean['target_variable']

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, stratify=df_clean['activity_segment'], random_state=42
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)

# 2. Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# 3. Verify no data leakage
print(f"Train shape: {X_train.shape}")
print(f"Validation shape: {X_val.shape}")
print(f"Test shape: {X_test.shape}")
print(f"Any overlap: {set(X_train.index) & set(X_test.index)}")  # Should be empty
```

### Phase 4: Baseline Model (Quick Validation)

**Estimated Time: 30 minutes**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Train baseline
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf.fit(X_train_scaled, y_train)

# Evaluate
y_pred = rf.predict(X_val_scaled)
y_pred_proba = rf.predict_proba(X_val_scaled)[:, 1]

print("Validation Performance:")
print(classification_report(y_val, y_pred))
print(f"ROC-AUC: {roc_auc_score(y_val, y_pred_proba):.4f}")

# Feature importance
importances = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 10 Features:")
print(importances.head(10))
```

---

## Visualization References

The following visualizations have been generated and saved to:
`/Users/txelusanchez/Documents/MBIT_MIA/Crypto Narrative Hunter - TFM/BMAD_TFM/data-collection/analysis/eda_plots/`

1. **01_performance_metrics_distribution.png**
   - ROI Percent distribution (left-skewed)
   - Win Rate distribution (extreme right-skew)
   - Sharpe Ratio distribution (concentrated)
   - Trade Frequency distribution (log scale)

2. **02_outlier_detection_boxplots.png**
   - Box plots for 6 key features
   - Clear visualization of outlier extent
   - Useful for identifying extreme values

3. **03_correlation_heatmap.png**
   - Correlation matrix of 14 key features
   - Highlights high correlations (red/blue)
   - Useful for multicollinearity detection

4. **04_feature_relationships.png**
   - ROI vs Sharpe Ratio (r=0.89)
   - Holding Period vs Diamond Hands (r=1.00)
   - Win Rate vs Portfolio Size (r=0.35)
   - HHI vs Gini coefficient

5. **05_narrative_exposure.png**
   - Distribution of DeFi exposure (among holders)
   - Distribution of AI exposure (among holders)
   - Distribution of Meme exposure (among holders)

---

## Summary Checklist

### Data Quality Status

- âœ… **No missing values** - Perfect completeness
- âœ… **Valid wallet addresses** - All conform to Ethereum standard
- âœ… **No duplicates** - Each wallet unique
- âœ… **Consistent data types** - 33 numeric, 1 object (address)
- âš ï¸ **Some out-of-range values** - 3 features need fixing
- âŒ **Extreme value errors** - PnL and trade size calculations broken
- âš ï¸ **Zero-variance features** - 2 features must be removed
- âš ï¸ **High sparsity** - 20 features >50% zeros
- âš ï¸ **Multicollinearity** - 5 highly correlated pairs

### ML Readiness Checklist

Before proceeding with model development:

- [ ] Remove gas_optimization_score (zero variance)
- [ ] Remove dex_diversity_score (zero variance)
- [ ] Remove diamond_hands_score (redundant)
- [ ] Fix total_pnl_usd calculation errors
- [ ] Fix avg_trade_size_usd calculation errors
- [ ] Clip volume_consistency to [0, 1]
- [ ] Decide on sparse feature handling strategy
- [ ] Apply log transformations to skewed features
- [ ] Create binary indicators for sparse features
- [ ] Create interaction features
- [ ] Handle multicollinearity (remove or regularize)
- [ ] Define target variable clearly
- [ ] Create stratified train/val/test splits
- [ ] Scale/normalize features
- [ ] Verify no data leakage
- [ ] Train baseline model
- [ ] Evaluate and iterate

---

## Conclusion

This dataset has **excellent structural quality** but requires **critical fixes** before it's ready for machine learning:

**Strengths:**
- Complete data (no missing values)
- Valid, unique wallet identifiers
- Rich feature set spanning performance, behavior, and portfolio characteristics
- Blockchain-aware metrics (gas, DEX usage, narratives)

**Critical Issues:**
- Calculation errors in PnL/trade size features
- Zero-variance features providing no information
- High sparsity (many features >90% zeros)
- Perfect multicollinearity in some feature pairs

**Estimated Time to ML-Ready: 2-4 hours**

With proper data cleaning and feature engineering, this dataset should be suitable for:
1. **Binary classification:** Successful vs unsuccessful wallets
2. **Multi-class classification:** Wallet behavior segments
3. **Regression:** Predicting ROI or risk-adjusted returns
4. **Clustering:** Discovering wallet archetypes
5. **Anomaly detection:** Identifying suspicious patterns

**Next Steps:**
1. Fix calculation errors in source pipeline
2. Apply cleaning script (Phase 1 above)
3. Engineer features (Phase 2 above)
4. Split data properly (Phase 3 above)
5. Train baseline model (Phase 4 above)
6. Iterate and improve

---

**Report Generated:** October 25, 2025
**Analyst:** Blockchain Data Analysis System
**Dataset Version:** wallet_features_master_20251022_195455.csv
**Total Wallets Analyzed:** 2,159
**Total Features Analyzed:** 34
